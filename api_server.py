from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
import sys
import pandas as pd
import json
import logging
from datetime import datetime
from typing import List, Optional, Dict
import hashlib
import asyncio
from pathlib import Path
import uuid

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯¼å…¥æ‚¨ç°æœ‰çš„RAGæ¨¡å—
try:
    from rag_doc_vect_embedding_csv import (
        vector_store, df_csv, Ollama_embeddings, 
        DocumentProcessor, init_directories
    )
    from rag_doc_vect_retriever import retriever
    from rag_doc_vect_agent_main import LLMModel, chain
    
    RAG_AVAILABLE = True
    logger.info("âœ… RAGç³»ç»Ÿæ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError as e:
    logger.error(f"âŒ RAGæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    RAG_AVAILABLE = False

app = FastAPI(
    title="ç°ä»£æ±½è½¦æ™ºèƒ½å®¢æœAPI",
    description="åŸºäºRAGæŠ€æœ¯çš„æ™ºèƒ½å®¢æœç³»ç»Ÿ",
    version="2024.1"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ===== æ•°æ®æ¨¡å‹å®šä¹‰ =====
class QueryRequest(BaseModel):
    question: str

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    stream: bool = False

class KnowledgeBaseCreate(BaseModel):
    name: str
    description: Optional[str] = ""
    chunk_method: Optional[str] = "naive"
    chunk_size: Optional[int] = 512
    chunk_overlap: Optional[int] = 50

class DocumentInfo(BaseModel):
    name: str
    content: str
    metadata: Optional[dict] = {}

# ===== å…¨å±€å˜é‡ =====
knowledge_bases = {}  # å­˜å‚¨çŸ¥è¯†åº“ä¿¡æ¯
documents_storage = {}  # å­˜å‚¨æ–‡æ¡£ä¿¡æ¯
current_kb_id = "default_kb"  # å½“å‰æ´»è·ƒçš„çŸ¥è¯†åº“ID
chat_tasks: Dict[str, dict] = {}  # å­˜å‚¨èŠå¤©ä»»åŠ¡çŠ¶æ€

# ===== åˆå§‹åŒ–å‡½æ•° =====
async def initialize_system():
    """åˆå§‹åŒ–ç³»ç»Ÿ"""
    try:
        if RAG_AVAILABLE:
            # åˆå§‹åŒ–ç›®å½•
            init_directories()
            
            # åˆ›å»ºé»˜è®¤çŸ¥è¯†åº“
            knowledge_bases["default_kb"] = {
                "id": "default_kb",
                "name": "ç°ä»£æ±½è½¦ä¸»çŸ¥è¯†åº“",
                "description": "åŒ…å«ç°æœ‰CSVæ•°æ®çš„çŸ¥è¯†åº“",
                "chunk_method": "naive",
                "chunk_size": 512,
                "chunk_overlap": 50,
                "document_count": 1,
                "chunk_count": len(df_csv) if 'df_csv' in globals() else 0,
                "status": "active",
                "created_at": datetime.now().isoformat()
            }
            
            # æ·»åŠ CSVæ–‡æ¡£ä¿¡æ¯
            documents_storage["csv_data"] = {
                "id": "csv_data",
                "name": "dev_csv(1).csv",
                "kb_id": "default_kb",
                "size": f"{len(df_csv)} æ¡è®°å½•" if 'df_csv' in globals() else "0 æ¡è®°å½•",
                "status": "completed",
                "chunk_num": len(df_csv) if 'df_csv' in globals() else 0,
                "created_at": datetime.now().isoformat(),
                "type": "csv"
            }
            
            logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        else:
            logger.warning("âš ï¸ RAGç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
            
    except Exception as e:
        logger.error(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")

# ===== è¾…åŠ©å‡½æ•° =====
def generate_kb_id(name: str) -> str:
    """ç”ŸæˆçŸ¥è¯†åº“ID"""
    return f"kb_{hashlib.md5(name.encode()).hexdigest()[:8]}"

def generate_doc_id(filename: str) -> str:
    """ç”Ÿæˆæ–‡æ¡£ID"""
    return f"doc_{hashlib.md5(filename.encode()).hexdigest()[:8]}"

async def process_csv_to_vector_store(csv_path: str, kb_id: str):
    """å¤„ç†CSVæ–‡ä»¶å¹¶æ·»åŠ åˆ°å‘é‡åº“"""
    try:
        if not RAG_AVAILABLE:
            return False
            
        # è¯»å–CSVæ–‡ä»¶
        df = pd.read_csv(csv_path, encoding='utf-8')
        logger.info(f"ğŸ“„ è¯»å–CSVæ–‡ä»¶: {len(df)} è¡Œæ•°æ®")
        
        # åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
        processor = DocumentProcessor()
        documents = []
        
        # å°†CSVè¡Œè½¬æ¢ä¸ºæ–‡æ¡£
        for i, row in df.iterrows():
            try:
                # æ„å»ºæ–‡æ¡£å†…å®¹
                content = ""
                if 'Title' in row and pd.notna(row['Title']):
                    content += f"æ ‡é¢˜: {row['Title']}\n"
                if 'desc' in row and pd.notna(row['desc']):
                    content += f"æè¿°: {row['desc']}\n"
                
                # æ·»åŠ å…¶ä»–åˆ—
                for col in df.columns:
                    if col not in ['Title', 'desc'] and pd.notna(row[col]):
                        content += f"{col}: {row[col]}\n"
                
                if content.strip():
                    from langchain_core.documents import Document
                    document = Document(
                        page_content=content.strip(),
                        metadata={
                            "source": csv_path,
                            "row_id": i,
                            "kb_id": kb_id,
                            "type": "csv_row"
                        }
                    )
                    documents.append(document)
                    
            except Exception as e:
                logger.warning(f"âš ï¸ å¤„ç†ç¬¬{i}è¡Œæ—¶å‡ºé”™: {e}")
                continue
        
        if documents:
            # ä½¿ç”¨DocumentProcessorå¤„ç†æ–‡æ¡£
            processor.process_and_store(documents, vector_store)
            logger.info(f"âœ… æˆåŠŸå¤„ç† {len(documents)} ä¸ªæ–‡æ¡£")
            return True
        else:
            logger.warning("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹")
            return False
            
    except Exception as e:
        logger.error(f"âŒ å¤„ç†CSVæ–‡ä»¶å¤±è´¥: {e}")
        return False

# ===== ç³»ç»ŸAPI =====
@app.get("/api/system/status")
async def system_status():
    """ç³»ç»ŸçŠ¶æ€æ£€æŸ¥"""
    try:
        status_info = {
            "status": "healthy",
            "service": "ç°ä»£æ±½è½¦æ™ºèƒ½åŠ©æ‰‹",
            "version": "2024.1",
            "rag_available": RAG_AVAILABLE,
            "total_knowledge_bases": len(knowledge_bases),
            "total_documents": len(documents_storage)
        }
        
        if RAG_AVAILABLE:
            try:
                # æ£€æŸ¥å‘é‡åº“çŠ¶æ€
                collection = vector_store._collection
                document_count = collection.count()
                status_info.update({
                    "mode": "production",
                    "vector_store_documents": document_count,
                    "csv_records": len(df_csv) if 'df_csv' in globals() else 0
                })
            except Exception as e:
                logger.warning(f"å‘é‡åº“æ£€æŸ¥å¤±è´¥: {e}")
                status_info["mode"] = "limited"
        else:
            status_info["mode"] = "mock"
        
        return {
            "success": True,
            "code": 200,
            "message": "ç³»ç»Ÿè¿è¡Œæ­£å¸¸",
            "data": status_info
        }
    except Exception as e:
        return {
            "success": False,
            "code": 500,
            "message": f"ç³»ç»Ÿæ£€æŸ¥å¤±è´¥: {str(e)}",
            "data": None
        }

# ===== çŸ¥è¯†åº“ç®¡ç†API =====
@app.get("/api/v1/datasets")
async def get_datasets(page: int = 1, page_size: int = 30):
    """è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        kb_list = list(knowledge_bases.values())
        
        # åˆ†é¡µå¤„ç†
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        paginated_kbs = kb_list[start_idx:end_idx]
        
        return {
            "code": 0,
            "message": "success",
            "data": paginated_kbs,
            "total": len(kb_list),
            "page": page,
            "page_size": page_size
        }
    except Exception as e:
        logger.error(f"è·å–çŸ¥è¯†åº“åˆ—è¡¨å¤±è´¥: {e}")
        return {
            "code": 500,
            "message": f"è·å–çŸ¥è¯†åº“å¤±è´¥: {str(e)}",
            "data": []
        }

@app.post("/api/v1/datasets")
async def create_dataset(background_tasks: BackgroundTasks, request: KnowledgeBaseCreate):
    """åˆ›å»ºçŸ¥è¯†åº“"""
    try:
        kb_id = generate_kb_id(request.name)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if kb_id in knowledge_bases:
            return {
                "code": 400,
                "message": "çŸ¥è¯†åº“åç§°å·²å­˜åœ¨",
                "data": None
            }
        
        # åˆ›å»ºçŸ¥è¯†åº“è®°å½•
        new_kb = {
            "id": kb_id,
            "name": request.name,
            "description": request.description,
            "chunk_method": request.chunk_method,
            "chunk_size": request.chunk_size,
            "chunk_overlap": request.chunk_overlap,
            "document_count": 0,
            "chunk_count": 0,
            "status": "active",
            "created_at": datetime.now().isoformat()
        }
        
        knowledge_bases[kb_id] = new_kb
        logger.info(f"âœ… åˆ›å»ºçŸ¥è¯†åº“: {request.name} (ID: {kb_id})")
        
        return {
            "code": 0,
            "message": "åˆ›å»ºæˆåŠŸ",
            "data": new_kb
        }
    except Exception as e:
        logger.error(f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {e}")
        return {
            "code": 500,
            "message": f"åˆ›å»ºçŸ¥è¯†åº“å¤±è´¥: {str(e)}",
            "data": None
        }

@app.delete("/api/v1/datasets/{dataset_id}")
async def delete_dataset(dataset_id: str):
    """åˆ é™¤çŸ¥è¯†åº“"""
    try:
        if dataset_id not in knowledge_bases:
            return {
                "code": 404,
                "message": "çŸ¥è¯†åº“ä¸å­˜åœ¨",
                "data": None
            }
        
        # åˆ é™¤ç›¸å…³æ–‡æ¡£
        docs_to_delete = [doc_id for doc_id, doc in documents_storage.items() 
                         if doc.get("kb_id") == dataset_id]
        
        for doc_id in docs_to_delete:
            del documents_storage[doc_id]
        
        # åˆ é™¤çŸ¥è¯†åº“
        kb_name = knowledge_bases[dataset_id]["name"]
        del knowledge_bases[dataset_id]
        
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤çŸ¥è¯†åº“: {kb_name} åŠ {len(docs_to_delete)} ä¸ªæ–‡æ¡£")
        
        return {
            "code": 0,
            "message": "åˆ é™¤æˆåŠŸ",
            "data": None
        }
    except Exception as e:
        logger.error(f"åˆ é™¤çŸ¥è¯†åº“å¤±è´¥: {e}")
        return {
            "code": 500,
            "message": f"åˆ é™¤å¤±è´¥: {str(e)}",
            "data": None
        }

# ===== æ–‡æ¡£ç®¡ç†API =====
@app.get("/api/v1/datasets/{dataset_id}/documents")
async def get_documents(dataset_id: str, page: int = 1, page_size: int = 30, keywords: str = ""):
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    try:
        if dataset_id not in knowledge_bases:
            return {
                "code": 404,
                "message": "çŸ¥è¯†åº“ä¸å­˜åœ¨",
                "data": []
            }
        
        # ç­›é€‰å±äºè¯¥çŸ¥è¯†åº“çš„æ–‡æ¡£
        kb_documents = [doc for doc in documents_storage.values() 
                       if doc.get("kb_id") == dataset_id]
        
        # å…³é”®è¯è¿‡æ»¤
        if keywords:
            kb_documents = [doc for doc in kb_documents 
                           if keywords.lower() in doc["name"].lower()]
        
        # åˆ†é¡µ
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        paginated_docs = kb_documents[start_idx:end_idx]
        
        return {
            "code": 0,
            "message": "success",
            "data": paginated_docs,
            "total": len(kb_documents),
            "page": page,
            "page_size": page_size
        }
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return {
            "code": 500,
            "message": f"è·å–æ–‡æ¡£å¤±è´¥: {str(e)}",
            "data": []
        }

@app.post("/api/v1/datasets/{dataset_id}/documents")
async def upload_document(dataset_id: str, background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£"""
    try:
        if dataset_id not in knowledge_bases:
            return {
                "code": 404,
                "message": "çŸ¥è¯†åº“ä¸å­˜åœ¨",
                "data": None
            }
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_extensions = {'.pdf', '.docx', '.txt', '.csv', '.xlsx'}
        file_extension = Path(file.filename).suffix.lower()
        
        if file_extension not in allowed_extensions:
            return {
                "code": 400,
                "message": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_extension}",
                "data": None
            }
        
        # ä¿å­˜æ–‡ä»¶
        os.makedirs("./data", exist_ok=True)
        file_path = f"./data/{file.filename}"
        
        content = await file.read()
        with open(file_path, "wb") as buffer:
            buffer.write(content)
        
        # åˆ›å»ºæ–‡æ¡£è®°å½•
        doc_id = generate_doc_id(file.filename)
        doc_info = {
            "id": doc_id,
            "name": file.filename,
            "kb_id": dataset_id,
            "file_path": file_path,
            "size": f"{len(content)} bytes",
            "status": "processing",
            "chunk_num": 0,
            "created_at": datetime.now().isoformat(),
            "type": file_extension[1:]  # å»æ‰ç‚¹å·
        }
        
        documents_storage[doc_id] = doc_info
        
        # å¼‚æ­¥å¤„ç†æ–‡æ¡£
        if RAG_AVAILABLE:
            background_tasks.add_task(process_uploaded_document, doc_id, file_path, dataset_id)
        
        # æ›´æ–°çŸ¥è¯†åº“æ–‡æ¡£è®¡æ•°
        knowledge_bases[dataset_id]["document_count"] += 1
        
        logger.info(f"ğŸ“ ä¸Šä¼ æ–‡æ¡£: {file.filename} åˆ°çŸ¥è¯†åº“ {dataset_id}")
        
        return {
            "code": 0,
            "message": "ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨å¤„ç†ä¸­",
            "data": doc_info
        }
    except Exception as e:
        logger.error(f"ä¸Šä¼ æ–‡æ¡£å¤±è´¥: {e}")
        return {
            "code": 500,
            "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}",
            "data": None
        }

async def process_uploaded_document(doc_id: str, file_path: str, kb_id: str):
    """åå°å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£"""
    try:
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æ–‡æ¡£: {doc_id}")
        
        file_extension = Path(file_path).suffix.lower()
        
        if file_extension == '.csv':
            # å¤„ç†CSVæ–‡ä»¶
            success = await process_csv_to_vector_store(file_path, kb_id)
            
            if success:
                # æ›´æ–°CSVè®°å½•æ•°
                df = pd.read_csv(file_path)
                documents_storage[doc_id]["chunk_num"] = len(df)
                documents_storage[doc_id]["status"] = "completed"
                
                # æ›´æ–°çŸ¥è¯†åº“åˆ‡ç‰‡è®¡æ•°
                knowledge_bases[kb_id]["chunk_count"] += len(df)
            else:
                documents_storage[doc_id]["status"] = "failed"
                
        elif file_extension == '.txt':
            # å¤„ç†æ–‡æœ¬æ–‡ä»¶
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            from langchain_core.documents import Document
            document = Document(
                page_content=content,
                metadata={
                    "source": file_path,
                    "kb_id": kb_id,
                    "type": "text"
                }
            )
            
            processor = DocumentProcessor()
            processor.process_and_store([document], vector_store)
            
            # ä¼°ç®—åˆ‡ç‰‡æ•°é‡
            chunk_count = len(content) // 500 + 1
            documents_storage[doc_id]["chunk_num"] = chunk_count
            documents_storage[doc_id]["status"] = "completed"
            knowledge_bases[kb_id]["chunk_count"] += chunk_count
            
        else:
            # å…¶ä»–æ–‡ä»¶ç±»å‹æš‚æ—¶æ ‡è®°ä¸ºå®Œæˆ
            documents_storage[doc_id]["status"] = "completed"
            documents_storage[doc_id]["chunk_num"] = 1
            knowledge_bases[kb_id]["chunk_count"] += 1
        
        logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_id}")
        
    except Exception as e:
        logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ {doc_id}: {e}")
        if doc_id in documents_storage:
            documents_storage[doc_id]["status"] = "failed"

@app.delete("/api/v1/datasets/{dataset_id}/documents")
async def delete_documents(dataset_id: str, document_ids: List[str]):
    """æ‰¹é‡åˆ é™¤æ–‡æ¡£"""
    try:
        if dataset_id not in knowledge_bases:
            return {
                "code": 404,
                "message": "çŸ¥è¯†åº“ä¸å­˜åœ¨",
                "data": None
            }
        
        deleted_count = 0
        for doc_id in document_ids:
            if doc_id in documents_storage and documents_storage[doc_id]["kb_id"] == dataset_id:
                # åˆ é™¤æ–‡ä»¶
                file_path = documents_storage[doc_id].get("file_path")
                if file_path and os.path.exists(file_path):
                    os.remove(file_path)
                
                # æ›´æ–°è®¡æ•°
                chunk_count = documents_storage[doc_id].get("chunk_num", 0)
                knowledge_bases[dataset_id]["chunk_count"] -= chunk_count
                knowledge_bases[dataset_id]["document_count"] -= 1
                
                del documents_storage[doc_id]
                deleted_count += 1
        
        return {
            "code": 0,
            "message": f"æˆåŠŸåˆ é™¤ {deleted_count} ä¸ªæ–‡æ¡£",
            "data": {"deleted_count": deleted_count}
        }
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
        return {
            "code": 500,
            "message": f"åˆ é™¤å¤±è´¥: {str(e)}",
            "data": None
        }

# ===== æ™ºèƒ½å¯¹è¯API =====
@app.post("/api/v1/chats_openai/{chat_id}/chat/completions")
async def chat_completions(chat_id: str, request: ChatRequest, background_tasks: BackgroundTasks):
    """OpenAIå…¼å®¹çš„å¯¹è¯API - ä½¿ç”¨å¼‚æ­¥ä»»åŠ¡å¤„ç†"""
    try:
        # è·å–ç”¨æˆ·æ¶ˆæ¯
        user_messages = [msg for msg in request.messages if msg.role == "user"]
        if not user_messages:
            raise HTTPException(status_code=400, detail="æ²¡æœ‰æ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯")
        
        user_question = user_messages[-1].content
        task_id = str(uuid.uuid4())
        
        logger.info(f"ğŸ’¬ æ”¶åˆ°å¯¹è¯è¯·æ±‚: {user_question[:50]}... (ä»»åŠ¡ID: {task_id})")
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        chat_tasks[task_id] = {
            "status": "processing",
            "result": None,
            "created_at": datetime.now().isoformat(),
            "question": user_question
        }
        
        # åå°å¤„ç†ä»»åŠ¡
        background_tasks.add_task(process_chat_task, task_id, user_question, request.model)
        
        # ç«‹å³è¿”å›ä»»åŠ¡IDå’ŒçŠ¶æ€
        return {
            "task_id": task_id,
            "status": "processing",
            "message": "æ­£åœ¨å¤„ç†ä¸­ï¼Œè¯·ä½¿ç”¨ä»»åŠ¡IDæŸ¥è¯¢ç»“æœ",
            "poll_url": f"/api/v1/chats_openai/task/{task_id}"
        }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºå¯¹è¯ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")

async def process_chat_task(task_id: str, question: str, model: str):
    """åå°å¤„ç†èŠå¤©ä»»åŠ¡"""
    try:
        logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†ä»»åŠ¡: {task_id}")
        
        # æ›´æ–°çŠ¶æ€ä¸ºæ£€ç´¢ä¸­
        chat_tasks[task_id]["status"] = "retrieving"
        chat_tasks[task_id]["message"] = "æ­£åœ¨æ£€ç´¢ç›¸å…³ä¿¡æ¯..."
        
        if RAG_AVAILABLE:
            try:
                # æ£€ç´¢é˜¶æ®µ
                logger.info(f"ğŸ” å¼€å§‹RAGæ£€ç´¢: {task_id}")
                reviews = await asyncio.get_event_loop().run_in_executor(
                    None, retriever.invoke, question
                )
                logger.info(f"ğŸ“š æ£€ç´¢åˆ° {len(reviews)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ: {task_id}")
                
                # æ›´æ–°çŠ¶æ€ä¸ºç”Ÿæˆä¸­
                chat_tasks[task_id]["status"] = "generating"
                chat_tasks[task_id]["message"] = "æ­£åœ¨ç”Ÿæˆå›ç­”..."
                
                # ç”Ÿæˆé˜¶æ®µ
                logger.info(f"ğŸ¤– å¼€å§‹ç”Ÿæˆå›ç­”: {task_id}")
                result = await asyncio.get_event_loop().run_in_executor(
                    None, lambda: chain.invoke({"reviews": reviews, "question": question})
                )
                
                # ä»»åŠ¡å®Œæˆ
                chat_tasks[task_id] = {
                    **chat_tasks[task_id],
                    "status": "completed",
                    "message": "å›ç­”ç”Ÿæˆå®Œæˆ",
                    "result": {
                        "id": f"chatcmpl-{task_id[:10]}",
                        "object": "chat.completion",
                        "created": int(datetime.now().timestamp()),
                        "model": model,
                        "choices": [
                            {
                                "index": 0,
                                "message": {
                                    "role": "assistant",
                                    "content": result
                                },
                                "finish_reason": "stop"
                            }
                        ],
                        "usage": {
                            "prompt_tokens": len(question.split()),
                            "completion_tokens": len(result.split()) if isinstance(result, str) else 50,
                            "total_tokens": len(question.split()) + (len(result.split()) if isinstance(result, str) else 50)
                        }
                    },
                    "completed_at": datetime.now().isoformat()
                }
                
                logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {task_id}")
                
            except Exception as rag_error:
                logger.error(f"âŒ RAGå¤„ç†å¤±è´¥ {task_id}: {rag_error}")
                chat_tasks[task_id] = {
                    **chat_tasks[task_id],
                    "status": "failed",
                    "message": f"å¤„ç†å¤±è´¥: {str(rag_error)}",
                    "error": str(rag_error),
                    "failed_at": datetime.now().isoformat()
                }
        else:
            # æ¨¡æ‹Ÿæ¨¡å¼
            await asyncio.sleep(2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            
            mock_answer = f"""æ„Ÿè°¢æ‚¨çš„å’¨è¯¢ï¼š"{question}"

æˆ‘æ˜¯ç°ä»£æ±½è½¦æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œå½“å‰ç³»ç»Ÿè¿è¡Œåœ¨æ¼”ç¤ºæ¨¡å¼ã€‚

å¦‚æœæ‚¨çš„é—®é¢˜æ˜¯å…³äºï¼š
â€¢ è½¦å‹ä¿¡æ¯ - æˆ‘å¯ä»¥ä¸ºæ‚¨ä»‹ç»ç°ä»£æ±½è½¦çš„ä¸»è¦è½¦å‹
â€¢ å”®åæœåŠ¡ - åŒ…æ‹¬ä¿å…»ã€ç»´ä¿®ã€ä¿ä¿®ç­‰ç›¸å…³æœåŠ¡
â€¢ è´­è½¦å’¨è¯¢ - ååŠ©æ‚¨äº†è§£è´­è½¦æµç¨‹å’Œæ”¿ç­–

ä¸ºäº†è·å¾—æœ€å‡†ç¡®çš„ä¿¡æ¯ï¼Œå»ºè®®æ‚¨ï¼š
1. è”ç³»å°±è¿‘çš„ç°ä»£æ±½è½¦4Såº—
2. æ‹¨æ‰“ç°ä»£æ±½è½¦å®¢æœçƒ­çº¿
3. è®¿é—®ç°ä»£æ±½è½¦å®˜æ–¹ç½‘ç«™

æœ‰ä»€ä¹ˆå…¶ä»–å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"""

            chat_tasks[task_id] = {
                **chat_tasks[task_id],
                "status": "completed",
                "message": "å›ç­”ç”Ÿæˆå®Œæˆï¼ˆæ¼”ç¤ºæ¨¡å¼ï¼‰",
                "result": {
                    "id": f"chatcmpl-{task_id[:10]}",
                    "object": "chat.completion",
                    "created": int(datetime.now().timestamp()),
                    "model": model,
                    "choices": [
                        {
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": mock_answer
                            },
                            "finish_reason": "stop"
                        }
                    ],
                    "usage": {
                        "prompt_tokens": len(question.split()),
                        "completion_tokens": len(mock_answer.split()),
                        "total_tokens": len(question.split()) + len(mock_answer.split())
                    }
                },
                "completed_at": datetime.now().isoformat()
            }
            
            logger.info(f"âœ… æ¼”ç¤ºä»»åŠ¡å®Œæˆ: {task_id}")
            
    except Exception as e:
        logger.error(f"âŒ ä»»åŠ¡å¤„ç†å¼‚å¸¸ {task_id}: {e}")
        chat_tasks[task_id] = {
            **chat_tasks.get(task_id, {}),
            "status": "failed",
            "message": f"ç³»ç»Ÿå¼‚å¸¸: {str(e)}",
            "error": str(e),
            "failed_at": datetime.now().isoformat()
        }

@app.get("/api/v1/chats_openai/task/{task_id}")
async def get_chat_task(task_id: str):
    """æŸ¥è¯¢èŠå¤©ä»»åŠ¡çŠ¶æ€"""
    try:
        if task_id not in chat_tasks:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        task_info = chat_tasks[task_id]
        
        # å¦‚æœä»»åŠ¡å®Œæˆè¶…è¿‡1å°æ—¶ï¼Œæ¸…ç†ä»»åŠ¡
        if task_info.get("status") in ["completed", "failed"]:
            completed_at = task_info.get("completed_at") or task_info.get("failed_at")
            if completed_at:
                from datetime import datetime, timedelta
                completed_time = datetime.fromisoformat(completed_at)
                if datetime.now() - completed_time > timedelta(hours=1):
                    del chat_tasks[task_id]
                    raise HTTPException(status_code=404, detail="ä»»åŠ¡å·²è¿‡æœŸ")
        
        return task_info
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æŸ¥è¯¢ä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æŸ¥è¯¢ä»»åŠ¡å¤±è´¥: {str(e)}")

# æ·»åŠ ç®€åŒ–çš„ä»»åŠ¡æŸ¥è¯¢è·¯å¾„ï¼ˆå…¼å®¹æ€§ï¼‰
@app.get("/api/v1/tasks/{task_id}")
async def get_task_simple(task_id: str):
    """ç®€åŒ–çš„ä»»åŠ¡æŸ¥è¯¢è·¯å¾„ï¼ˆå…¼å®¹æ€§ï¼‰"""
    return await get_chat_task(task_id)

@app.get("/api/v1/chats_openai/tasks")
async def list_chat_tasks(limit: int = 10):
    """åˆ—å‡ºæœ€è¿‘çš„èŠå¤©ä»»åŠ¡ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
    try:
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„ä»»åŠ¡
        sorted_tasks = sorted(
            chat_tasks.items(),
            key=lambda x: x[1].get("created_at", ""),
            reverse=True
        )
        
        return {
            "total": len(chat_tasks),
            "tasks": [
                {
                    "task_id": task_id,
                    "status": task_info["status"],
                    "question": task_info.get("question", "")[:50] + "..." if len(task_info.get("question", "")) > 50 else task_info.get("question", ""),
                    "created_at": task_info.get("created_at"),
                    "message": task_info.get("message", "")
                }
                for task_id, task_info in sorted_tasks[:limit]
            ]
        }
    except Exception as e:
        logger.error(f"åˆ—å‡ºä»»åŠ¡å¤±è´¥: {e}")
        return {"total": 0, "tasks": []}

# ===== æ£€ç´¢å’Œæµ‹è¯•API =====
@app.post("/api/retriever/test")
async def test_retriever(request: QueryRequest):
    """æµ‹è¯•æ£€ç´¢åŠŸèƒ½"""
    try:
        logger.info(f"ğŸ” æµ‹è¯•æ£€ç´¢: {request.question}")
        
        if not RAG_AVAILABLE:
            return {
                "query": request.question,
                "results": [
                    {
                        "content": f"æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ1 - å…³äº'{request.question}'çš„ç›¸å…³ä¿¡æ¯", 
                        "metadata": {"source": "demo", "score": 0.95}
                    },
                    {
                        "content": f"æ¨¡æ‹Ÿæ£€ç´¢ç»“æœ2 - æ›´å¤šå…³äº'{request.question}'çš„è¯¦ç»†ä¿¡æ¯", 
                        "metadata": {"source": "demo", "score": 0.87}
                    }
                ],
                "count": 2,
                "status": "mock_mode"
            }
        
        # ä½¿ç”¨çœŸå®æ£€ç´¢å™¨
        results = retriever.get_relevant_documents(request.question)
        
        return {
            "query": request.question,
            "results": [
                {
                    "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                    "metadata": doc.metadata
                } for doc in results
            ],
            "count": len(results),
            "status": "success"
        }
    except Exception as e:
        logger.error(f"æ£€ç´¢æµ‹è¯•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/retriever/stats")
async def get_vector_stats():
    """å‘é‡æ•°æ®åº“ç»Ÿè®¡"""
    try:
        if not RAG_AVAILABLE:
            return {
                "document_count": 0,
                "csv_records": 0,
                "knowledge_bases": len(knowledge_bases),
                "total_documents": len(documents_storage),
                "status": "mock_mode"
            }
        
        collection = vector_store._collection
        vector_count = collection.count()
        
        return {
            "document_count": vector_count,
            "csv_records": len(df_csv) if 'df_csv' in globals() else 0,
            "knowledge_bases": len(knowledge_bases),
            "total_documents": len(documents_storage),
            "status": "active",
            "details": {
                "knowledge_bases": list(knowledge_bases.keys()),
                "document_types": list(set(doc.get("type", "unknown") for doc in documents_storage.values()))
            }
        }
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        return {
            "document_count": 0,
            "csv_records": 0,
            "knowledge_bases": 0,
            "total_documents": 0,
            "status": "error",
            "error": str(e)
        }

# ===== å…¼å®¹æ€§API =====
@app.post("/api/chat/send")
async def send_message(request: QueryRequest):
    """åŸæœ‰çš„å¯¹è¯APIï¼ˆå…¼å®¹æ€§ï¼‰"""
    try:
        logger.info(f"ğŸ’¬ å…¼å®¹APIæ”¶åˆ°æ¶ˆæ¯: {request.question}")
        
        if not RAG_AVAILABLE:
            return {
                "answer": f"æ‚¨å¥½ï¼æ‚¨è¯¢é—®çš„æ˜¯ï¼š{request.question}ã€‚å½“å‰ç³»ç»Ÿè¿è¡Œåœ¨æ¼”ç¤ºæ¨¡å¼ï¼Œè¯·è”ç³»ç®¡ç†å‘˜å¯ç”¨å®Œæ•´åŠŸèƒ½ã€‚",
                "reviews": [],
                "status": "mock_mode"
            }
        
        reviews = retriever.invoke(request.question)
        result = chain.invoke({"reviews": reviews, "question": request.question})
        
        return {
            "answer": result,
            "reviews": [doc.page_content for doc in reviews],
            "status": "success"
        }
    except Exception as e:
        logger.error(f"å…¼å®¹APIå¤„ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy", 
        "service": "ç°ä»£æ±½è½¦æ™ºèƒ½å®¢æœAPI",
        "timestamp": datetime.now().isoformat(),
        "rag_available": RAG_AVAILABLE
    }

@app.get("/api/system/info")
async def get_system_info():
    """ç³»ç»Ÿä¿¡æ¯"""
    return {
        "python_files": {
            "embedding": os.path.exists("rag_doc_vect_embedding_csv.py"),
            "retriever": os.path.exists("rag_doc_vect_retriever.py"),
            "agent": os.path.exists("rag_doc_vect_agent_main.py")
        },
        "data_files": {
            "csv_file": os.path.exists("dev_csv(1).csv"),
            "data_dir": os.path.exists("data"),
            "knowledge_db": os.path.exists("knowledge_db")
        },
        "rag_available": RAG_AVAILABLE,
        "system_stats": {
            "knowledge_bases": len(knowledge_bases),
            "documents": len(documents_storage),
            "vector_store_available": RAG_AVAILABLE
        }
    }

# ===== å¯åŠ¨æ—¶åˆå§‹åŒ– =====
@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œ"""
    logger.info("ğŸš€ ç°ä»£æ±½è½¦æ™ºèƒ½å®¢æœAPIå¯åŠ¨ä¸­...")
    await initialize_system()
    logger.info("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ç°ä»£æ±½è½¦æ™ºèƒ½å®¢æœAPIå¯åŠ¨ä¸­...")
    print("ğŸ“ APIæ–‡æ¡£: http://localhost:8000/docs")
    print("ğŸ” å¥åº·æ£€æŸ¥: http://localhost:8000/api/system/status")
    print("ğŸ’¬ æµ‹è¯•å¯¹è¯: http://localhost:8000/docs#/default/chat_completions_api_v1_chats_openai__chat_id__chat_completions_post")
    uvicorn.run(app, host="0.0.0.0", port=8000)